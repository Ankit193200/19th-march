{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ea7482-8619-4223-bda1-a84d31152704",
   "metadata": {},
   "source": [
    "# Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max Scaling: A technique that transforms features by scaling them to a specific range, often [0, 1].\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # Original data\n",
    "data = [[1, 2], [3, 4], [5, 6]]\n",
    " \n",
    "    # Applying Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nMin-Max Scaled Data:\")\n",
    "print(scaled_data)\n",
    "# Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "Unit Vector Technique: Scaling features to have a unit norm (length 1).\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    # Original data\n",
    "data = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "    # Applying Unit Vector scaling\n",
    "scaler = Normalizer()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nUnit Vector Scaled Data:\")\n",
    "print(scaled_data)\n",
    "# Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "PCA (Principal Component Analysis): A technique for dimensionality reduction that identifies the most important features and projects the data onto a lower-dimensional subspace.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "    # Original data\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "    # Applying PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nReduced Dimension Data:\")\n",
    "print(reduced_data)\n",
    "# Q4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Relationship: PCA can be used for feature extraction by transforming the original features into a new set of uncorrelated features (principal components).\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "    # Original data\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "    # Applying PCA for feature extraction\n",
    "pca = PCA(n_components=2)\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nExtracted Features:\")\n",
    "print(extracted_features)\n",
    "# Q5: You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "Explanation: Min-Max scaling would be applied to ensure that all features are on a similar scale, preventing one feature from dominating others. For example, if the price feature has a range of $5 to $100 and the rating feature has a range of 1 to 5, Min-Max scaling would bring both features to a common scale, say [0, 1].\n",
    "# Q6: You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Explanation: PCA can be applied to identify the most significant features (principal components) in the dataset. By selecting a reduced number of principal components, the dimensionality of the dataset is reduced while retaining the most important information, which can be useful for predicting stock prices.\n",
    "# Q7: For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "Original Range: 1 to 20\n",
    "Min-Max Scaling:\n",
    "python\n",
    "Copy code\n",
    "data = [1, 5, 10, 15, 20]\n",
    "new_min, new_max = -1, 1\n",
    "\n",
    "min_val, max_val = min(data), max(data)\n",
    "scaled_data = [(x - min_val) / (max_val - min_val) * (new_max - new_min) + new_min for x in data]\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nMin-Max Scaled Data (-1 to 1):\")\n",
    "print(scaled_data)\n",
    "# Q8: For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Decision: The number of principal components to retain depends on the desired level of explained variance. Typically, a percentage of variance (e.g., 95%) is set as a threshold. The number of components is chosen to capture enough variance.\n",
    "\n",
    "Example Code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "    # Assuming 'data' contains the dataset\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "    # Determine the number of components to retain based on explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "    # Choose the number of components to retain (e.g., 95% of variance)\n",
    "n_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Number of Components to Retain:\", n_components_to_retain)\n",
    "In this example, the number of components to retain is chosen based on capturing 95% of the cumulative explained variance. Adjust the threshold based on specific project requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b61502-3e5d-498f-83cc-37d186ec885e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
